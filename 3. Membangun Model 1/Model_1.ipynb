{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPtvqrHtStbu"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDA3l2lAYJDh"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQS-1iUpQFfY",
        "outputId": "82b14ba5-2034-4e67-de56-40bc62842803"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9o6ExVsTsTh"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjpz7sizTvrJ"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXIvHCVYTyrJ"
      },
      "source": [
        "data_set_dir = \"/content/drive/My Drive/Machine Learning/Tubes/Dataset\"\n",
        "transform = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWAA2v_kSxyj"
      },
      "source": [
        "#Step 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhq2loYHX9L7"
      },
      "source": [
        "dataset = torchvision.datasets.ImageFolder(data_set_dir, transform=transform)\n",
        "batch_size = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aADkj8cKYTdE",
        "outputId": "37ce9003-7e72-45ea-e2ed-927a73fadce1"
      },
      "source": [
        "print(len(dataset))\n",
        "from torch.utils.data import random_split"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHY9s5QWYV6k"
      },
      "source": [
        "val_size = 800\n",
        "trains_size = len(dataset) - val_size\n",
        "train_dataset, val_dataset = random_split(dataset, [trains_size, val_size])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CDWFP_ZYYla",
        "outputId": "e93c7ce1-2ca6-45e1-c7d3-4a097237d8bd"
      },
      "source": [
        "print(len(train_dataset))\n",
        "print(len(val_dataset))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3294\n",
            "800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H97xQ7REYe0a"
      },
      "source": [
        "from torch.utils.data.dataloader import DataLoader"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erVqKtFZYhFO"
      },
      "source": [
        "train_dl = DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "val_dl = DataLoader(val_dataset, batch_size*2)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwItxy2sYl0L"
      },
      "source": [
        "class ImageClassification(nn.Module):\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)\n",
        "        loss = F.cross_entropy(out, labels)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)\n",
        "        loss = F.cross_entropy(out, labels)\n",
        "        acc = accuracy(out, labels)\n",
        "\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "\n",
        "\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0euK3fdIYo8M"
      },
      "source": [
        "class MaskClassification(ImageClassification):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "\n",
        "            # LAYER 1\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=(2, 2), stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=(2, 2), stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.25),\n",
        "\n",
        "            # LAYER 2\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=(2, 2), stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=(2, 2), stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.25),\n",
        "\n",
        "            # LAYER 3\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=(2, 2), stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=(2, 2), stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            # LAST LAYER\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9whvTBgYrT4"
      },
      "source": [
        "model = MaskClassification()\n",
        "\n",
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in train_loader:\n",
        "            loss = model.train_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "\n",
        "    return history"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms8BMBILYtWy"
      },
      "source": [
        "num_of_epoch = 20\n",
        "optimizer_func = torch.optim.Adam\n",
        "lr = 0.001"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUJ2RYI2YvR3",
        "outputId": "e19aebbc-db74-4fd1-cdce-23578b2c49bb"
      },
      "source": [
        "history = fit(num_of_epoch, lr, model, train_dl, val_dl, optimizer_func)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0], train_loss: 0.7490, val_loss: 0.6919, val_acc: 0.5214\n",
            "Epoch [1], train_loss: 0.6481, val_loss: 0.4989, val_acc: 0.7589\n",
            "Epoch [2], train_loss: 0.4225, val_loss: 0.3238, val_acc: 0.8786\n",
            "Epoch [3], train_loss: 0.3199, val_loss: 0.2905, val_acc: 0.8952\n",
            "Epoch [4], train_loss: 0.2863, val_loss: 0.2349, val_acc: 0.9107\n",
            "Epoch [5], train_loss: 0.2158, val_loss: 0.1654, val_acc: 0.9387\n",
            "Epoch [6], train_loss: 0.1685, val_loss: 0.1509, val_acc: 0.9423\n",
            "Epoch [7], train_loss: 0.1545, val_loss: 0.1311, val_acc: 0.9536\n",
            "Epoch [8], train_loss: 0.1247, val_loss: 0.1828, val_acc: 0.9411\n",
            "Epoch [9], train_loss: 0.1094, val_loss: 0.1459, val_acc: 0.9560\n",
            "Epoch [10], train_loss: 0.0922, val_loss: 0.1565, val_acc: 0.9452\n",
            "Epoch [11], train_loss: 0.0848, val_loss: 0.1921, val_acc: 0.9500\n",
            "Epoch [12], train_loss: 0.0738, val_loss: 0.2013, val_acc: 0.9458\n",
            "Epoch [13], train_loss: 0.0655, val_loss: 0.1676, val_acc: 0.9554\n",
            "Epoch [14], train_loss: 0.0565, val_loss: 0.1294, val_acc: 0.9625\n",
            "Epoch [15], train_loss: 0.0418, val_loss: 0.1469, val_acc: 0.9560\n",
            "Epoch [16], train_loss: 0.0378, val_loss: 0.1690, val_acc: 0.9595\n",
            "Epoch [17], train_loss: 0.0413, val_loss: 0.1418, val_acc: 0.9637\n",
            "Epoch [18], train_loss: 0.0399, val_loss: 0.1742, val_acc: 0.9589\n",
            "Epoch [19], train_loss: 0.0285, val_loss: 0.1741, val_acc: 0.9625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SPM5D2SYxGW"
      },
      "source": [
        "import pathlib\n",
        "root=pathlib.Path(\"/content/drive/My Drive/Machine Learning/Tubes/Dataset\")\n",
        "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz0pA4CYYy4k",
        "outputId": "759b37b1-45cd-401b-bac1-df61359c7c5f"
      },
      "source": [
        "print(classes)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['with_mask', 'without_mask']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSc0difRY04u"
      },
      "source": [
        "#Transforms\n",
        "\n",
        "transformer=transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
        "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
        "                        [0.5,0.5,0.5])\n",
        "])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz6SeioeY26H"
      },
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Prediction Function\n",
        "\n",
        "def prediction(img_path,transformer):\n",
        "    \n",
        "    image=Image.open(img_path)\n",
        "    \n",
        "    image_tensor=transformer(image).float()\n",
        "    \n",
        "    \n",
        "    image_tensor=image_tensor.unsqueeze_(0)\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        image_tensor.cuda()\n",
        "        \n",
        "    input=Variable(image_tensor)\n",
        "    \n",
        "    \n",
        "    output=model(input)\n",
        "    \n",
        "    index=output.data.numpy().argmax()\n",
        "    \n",
        "    pred=classes[index]\n",
        "    \n",
        "    return pred"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JZFwXhZUY5Az",
        "outputId": "bb3d3f6b-b4e5-41b1-958d-34fa35ea082b"
      },
      "source": [
        "# How to predict\n",
        "\n",
        "# With Mask\n",
        "image_path1 = \"/content/drive/My Drive/maksssksksss6.png\"\n",
        "result1 = prediction(image_path1,transformer)\n",
        "\n",
        "# Mask weared incorrect\n",
        "# image_path2 = \"../input/face-mask-detection/Dataset/mask_weared_incorrect/1002.png\"\n",
        "# result2 = prediction(image_path2,transformer)\n",
        "\n",
        "# Without Mask\n",
        "# image_path3 = \"../input/face-mask-detection/Dataset/without_mask/1006.png\"\n",
        "# result3 = prediction(image_path3,transformer)\n",
        "\n",
        "result1"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'with_mask'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaeDVMC8Y7G6"
      },
      "source": [
        "# Save the model\n",
        "\n",
        "import torch\n",
        "FILE = \"/content/drive/My Drive/Machine Learning/Tubes/Model 1/model_1.h5\"\n",
        "torch.save(model, FILE)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BigCbqxemqu"
      },
      "source": [
        "import json\n",
        "\n",
        "target_dir = '/content/drive/My Drive/Machine Learning/Tubes/Model 1'\n",
        "\n",
        "# simpan history model 1\n",
        "history_dict = history.history\n",
        "json.dump(history_dict, open(target_dir + '/history_model_1.json', 'w'))\n",
        "\n",
        "# simpan model 11 dan weight-nya\n",
        "model.save(target_dir + '/model_1.h5')\n",
        "model.save_weights(target_dir + '/model_1_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}